
# Wikipedia Metin Ã–n Ä°ÅŸleme ve GÃ¶rselleÅŸtirme Projesi

Bu proje, Wikipedia metin verilerini iÃ§eren bir veri seti Ã¼zerinde doÄŸal dil iÅŸleme (NLP) teknikleri kullanarak metin Ã¶n iÅŸleme ve gÃ¶rselleÅŸtirme adÄ±mlarÄ±nÄ± iÃ§ermektedir.

## ğŸ“ Proje AÃ§Ä±klamasÄ±

Projenin temel amacÄ±, ham metin verisini yapÄ±landÄ±rÄ±lmÄ±ÅŸ ve analiz edilebilir bir formata dÃ¶nÃ¼ÅŸtÃ¼rmektir. Bu sÃ¼reÃ§te aÅŸaÄŸÄ±daki adÄ±mlar uygulanmÄ±ÅŸtÄ±r:
- Metin verisinin standartlaÅŸtÄ±rÄ±lmasÄ± (kÃ¼Ã§Ã¼k harfe Ã§evirme, noktalama iÅŸaretleri ve sayÄ±larÄ±n kaldÄ±rÄ±lmasÄ±).
- Metin iÃ§inde sÄ±kÃ§a geÃ§en ancak anlamsal deÄŸeri dÃ¼ÅŸÃ¼k olan "stopwords" (etkisiz kelimeler) ve Ã§ok seyrek kullanÄ±larak gÃ¼rÃ¼ltÃ¼ yaratan kelimelerin temizlenmesi.
- Kelimelerin kÃ¶klerine indirgenmesi (lemmatization).
- TemizlenmiÅŸ veri Ã¼zerinden kelime frekanslarÄ±nÄ±n hesaplanmasÄ± ve gÃ¶rselleÅŸtirilmesi.

TÃ¼m bu iÅŸlemler, projenin sonunda `wiki_processing` adÄ±nda tek bir fonksiyonda birleÅŸtirilmiÅŸtir.

## ğŸ“ Veri Seti

Proje, `wiki_data.csv` adlÄ± bir veri setini kullanmaktadÄ±r. Bu dosyanÄ±n, Jupyter Notebook (`wiki.ipynb`) dosyasÄ±yla aynÄ± dizinde olmasÄ± gerekmektedir. Veri seti, her satÄ±rda bir Wikipedia makalesinin metnini iÃ§eren 'text' adÄ±nda bir sÃ¼tuna sahiptir.

## ğŸ› ï¸ KullanÄ±lan Teknolojiler ve KÃ¼tÃ¼phaneler

- **Python 3.x**
- **Jupyter Notebook**
- **Pandas:** Veri manipÃ¼lasyonu ve CSV iÅŸlemleri iÃ§in.
- **NLTK (Natural Language ToolKit):** Stopwords ve lemmatization iÅŸlemleri iÃ§in.
- **TextBlob:** Lemmatization iÃ§in.
- **WordCloud:** Kelime bulutu gÃ¶rselleÅŸtirmesi iÃ§in.
- **Matplotlib & Seaborn:** Grafik Ã§izimleri iÃ§in.

## âš™ï¸ Kurulum ve BaÄŸÄ±mlÄ±lÄ±klar

Projeyi Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce gerekli kÃ¼tÃ¼phaneleri yÃ¼klemeniz gerekmektedir.

```bash
pip install pandas numpy matplotlib seaborn nltk textblob wordcloud
```

AyrÄ±ca, NLTK ve TextBlob kÃ¼tÃ¼phaneleri iÃ§in ek kaynaklarÄ± indirmeniz gerekebilir:
```python
import nltk
from textblob import Word

# NLTK iÃ§in stopwords ve wordnet verilerini indirme
nltk.download('stopwords')
nltk.download('wordnet')
```

## ğŸš€ KullanÄ±m

Projedeki tÃ¼m adÄ±mlar, `wiki_processing` fonksiyonu altÄ±nda toplanmÄ±ÅŸtÄ±r. Bu fonksiyon, ham metin verisini alÄ±p iÅŸler ve isteÄŸe baÄŸlÄ± olarak gÃ¶rselleÅŸtirmeleri yapar.

**Fonksiyonun Docstring'i:**
```python
def wiki_processing(text, Barplot=False, wordcloud=False):
    """ Wiki metinlerini iÅŸleme fonksiyonu.
    Args:
        text (str): Ä°ÅŸlenecek metin.
        Barplot (bool): Kelime frekanslarÄ±nÄ± barplot olarak gÃ¶sterir.
        wordcloud (bool): Kelime bulutunu gÃ¶sterir.
    Returns:
        pd.DataFrame: Ä°ÅŸlenmiÅŸ metin verisi.
    """
```

**Ã–rnek KullanÄ±m:**

```python
import pandas as pd
from warnings import filterwarnings

# Gerekli ayarlar ve kÃ¼tÃ¼phane indirmeleri
filterwarnings('ignore')
# nltk.download('...') vs.

# Veriyi yÃ¼kleme
df = pd.read_csv("wiki_data.csv", index_col=0)

# Fonksiyonu Ã§aÄŸÄ±rarak veriyi iÅŸleme
# Sadece metin Ã¶n iÅŸleme yapmak iÃ§in:
processed_text = wiki_processing(df["text"])

# Hem Ã¶n iÅŸleme hem de gÃ¶rselleri oluÅŸturmak iÃ§in:
processed_text_with_plots = wiki_processing(df["text"], Barplot=True, wordcloud=True)

print(processed_text_with_plots.head())
```
**Not:** Notebook'unuzdaki son hÃ¼crede `wiki_processing` fonksiyonunu Ã§aÄŸÄ±rdÄ±ÄŸÄ±nÄ±zda bir `NameError` almÄ±ÅŸsÄ±nÄ±z. Fonksiyonu tanÄ±mladÄ±ÄŸÄ±nÄ±z hÃ¼creyi Ã§alÄ±ÅŸtÄ±rdÄ±ktan sonra Ã§aÄŸÄ±rma hÃ¼cresini Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zda bu hata dÃ¼zelecektir.

## ğŸ“Š GÃ¶rsel Ã‡Ä±ktÄ±lar

Bu kÄ±sma, kodunuzun Ã¼rettiÄŸi gÃ¶rsellerin ekran gÃ¶rÃ¼ntÃ¼lerini ekleyebilirsiniz.

### Kelime FrekanslarÄ± Barplot GrafiÄŸi
*FrekansÄ± 8000'den fazla olan kelimelerin gÃ¶rseli.*

![Barplot GÃ¶rseli](path/to/barplot_image.png)

### Kelime Bulutu (WordCloud)
*Ä°ÅŸlenmiÅŸ metindeki en popÃ¼ler kelimelerin gÃ¶rsel temsili.*

![WordCloud GÃ¶rseli](path/to/wordcloud_image.png)

---

UmarÄ±m bu iÃ§erikler GitHub projenizi daha profesyonel ve anlaÅŸÄ±lÄ±r bir ÅŸekilde sunmanÄ±za yardÄ±mcÄ± olur. BaÅŸarÄ±lar!
